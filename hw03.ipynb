{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORwunP9a65qyDQiKguLB67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khorunj/3hw/blob/main/hw03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzbagbVh35Zw",
        "outputId": "1184f33d-2f05-4f56-87f5-6b0cd36f94cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N876ARj538Vy",
        "outputId": "0cfce215-7893-4d85-fbb8-8806f20ca6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "lzUuo2Qz38fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'http://quotes.toscrape.com'\n",
        "\n",
        "def get_urls():\n",
        "  urls_list = ['http://quotes.toscrape.com']\n",
        "  next_page = 'http://quotes.toscrape.com'\n",
        "  while next_page:\n",
        "    html_doc = requests.get(next_page)\n",
        "    soup = BeautifulSoup(html_doc.text, 'html.parser')\n",
        "    content = soup.select('nav ul.pager li.next a')\n",
        "\n",
        "\n",
        "    for link in content:\n",
        "        urls_list.append(URL + link['href'])\n",
        "    next_page=soup.select_one('nav ul.pager li.next a')\n",
        "    if next_page:\n",
        "      next_page = URL + next_page['href']\n",
        "  return urls_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QuV5JzvB38j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "4OGkyPB8_-pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quotes=[]\n",
        "authors_url=[]\n",
        "authors=[]\n",
        "for link in get_urls():\n",
        "  html_doc = requests.get(link)\n",
        "  soup = BeautifulSoup(html_doc.text, 'html.parser')\n",
        "  content = soup.select('div[class=container] div[class=row] div[class=col-md-8] div[class=quote]')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for li in content:\n",
        "    quote_text = li.find('span', attrs={'class':'text'}).text.strip()\n",
        "    author = li.find('small', attrs={'class':'author'}).text.strip()\n",
        "    tags_element = li.find('div', class_='tags')\n",
        "    tags_meta = tags_element.find('meta', attrs={'class':'keywords'})\n",
        "    tags = tags_meta['content'].split(',') if tags_meta else []\n",
        "    quote_data = {\n",
        "        \"tags\" : tags,\n",
        "        \"author\" : author,\n",
        "        \"quote\" : quote_text\n",
        "    }\n",
        "    quotes.append(quote_data)\n",
        "    abouts=li.select_one('a')['href']\n",
        "    authors_url.append(URL+abouts)\n",
        "\n",
        "json_data = json.dumps(quotes, indent=4)\n",
        "with open('quotes.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "for page in authors_url:\n",
        "  html_doc = requests.get(page)\n",
        "  soup = BeautifulSoup(html_doc.text, 'html.parser')\n",
        "  content = soup.select('div[class=container] div[class=author-details]')\n",
        "  fullname = soup.find('h3', class_='author-title').text.strip()\n",
        "  born_date = soup.find('span', class_='author-born-date').text.strip()\n",
        "  born_location = soup.find('span', class_='author-born-location').text.strip()\n",
        "  description = soup.find('div', class_='author-description').text.strip()\n",
        "  authors_data = {\n",
        "      \"fullname\": fullname,\n",
        "      \"born_date\": born_date,\n",
        "      \"born_location\": born_location,\n",
        "      \"description\": description\n",
        "  }\n",
        "  authors.append(authors_data)\n",
        "json_data = json.dumps(authors, indent=4)\n",
        "with open('authors.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_Cm8UPA__56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}